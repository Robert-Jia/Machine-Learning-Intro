{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import mean_squared_error\n",
                "%matplotlib inline\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the data from the file `backprop.csv`\n",
                "df = pd.read_csv('backprop.csv')\n",
                "\n",
                "# Assign the predictor and response variables to x and y\n",
                "x = df.x.values.reshape(-1,1)\n",
                "y = df.y.values.reshape(-1,1)\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the weights, but keep the random seed as 310 for reproducible results\n",
                "np.random.seed(310)\n",
                "\n",
                "# W is a list that contains both w1 and w2\n",
                "# W = [w1,w2]\n",
                "W = [np.random.randn(1, 1), np.random.randn(1, 1)]\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining the activation function and the neural network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the activation function\n",
                "def A(x):\n",
                "    return np.sin(x)\n",
                "\n",
                "# Function to compute the derivative of the activation function\n",
                "def der_A(x):\n",
                "    return np.cos(x)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to define the neural network\n",
                "def neural_network(W, x):\n",
                "    # W is a list of the two weights (w1,w2) of your neural network\n",
                "    # x is the input to the neural network\n",
                "    w1 = W[0]\n",
                "    w2 = W[1]\n",
                "    '''\n",
                "    Compute h1, h2,a1 and y\n",
                "    h1 is the dot product of the input and weight\n",
                "    To compute h2, first use the activation function on h1, then multiply by w2\n",
                "    Finally, use the activation function on h2 to compute y\n",
                "    Return all three values which you will use to compute derivatives later\n",
                "    '''\n",
                "    h1 = np.dot(x,w1)\n",
                "    a1 = np.sin(h1)\n",
                "    h2 = np.dot(a1,w2)\n",
                "    y = np.sin(h2)\n",
                "    \n",
                "    # Remember that we return all affine transformations and the output\n",
                "    return h1,h2,a1,y\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building the chain-rule components "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ Look at the schematic in the instructions. If we consider the first weight, what is $\\ \\frac{\\partial h_1}{\\partial w_1}$?\n",
                "\n",
                "#### A. $x$\n",
                "#### B. $f(x)$\n",
                "#### C. $f'(x)$\n",
                "#### D. $w_1x$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option C, put 'C')\n",
                "answer1 = '___'\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of affine \n",
                "# (particular neuron) wrt corresponding weight w\n",
                "\n",
                "def dhdw(var):\n",
                "    '''\n",
                "    The derivative of the affine wrt the preceding weight is just \n",
                "    the activation of the previous neuron.\n",
                "    Note, account for the case where the input layer has no activation \n",
                "    layers associated with it. i.e return the same value if it is the first weight \n",
                "    '''\n",
                "    return ___\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of activation wrt affine\n",
                "def dadh(h):\n",
                "    '''\n",
                "    This is the derivative of the output of the activation function \n",
                "    wrt the affine transformation.\n",
                "    Return the derivative of the affine transformation\n",
                "    '''\n",
                "    return ___\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of output wrt activation\n",
                "def dyda(h):\n",
                "    '''\n",
                "    This is the function to compute the partial derivative of the output \n",
                "    wrt the activation.\n",
                "    NOTE: It returns a constant (i.e. 1) for our simple neural network\n",
                "    because y=a2, however it may not be the case in general\n",
                "    '''\n",
                "    return ___\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of affine\n",
                "# wrt the previous activation\n",
                "def dhda(w):\n",
                "    '''\n",
                "    The derivative of the affine wrt the previous activation is the weight\n",
                "    '''\n",
                "    return ___\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of loss with respect to y\n",
                "def dldy(y_pred,y):\n",
                "    '''\n",
                "    Since our loss function is the MSE,\n",
                "    the partial derivative of L wrt y will be 2*(y_pred - y), \n",
                "    for all predictions and response\n",
                "    '''\n",
                "    return ___\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ Look at the schematic in the instructions. What is the difference between $h_i$ and $a_i$ for a given layer $i$?\n",
                "\n",
                "#### A. $h_i$ and $a_i$ are one and the same\n",
                "#### B. $h_i$ is the affine transformation on inputs from layer $i-1$ and $a_i$ is the activation over $h_i$\n",
                "#### C. $a_i$ is the affine transformation on inputs from layer $i-1$ and $h_i$ is the activation over $a_i$\n",
                "#### D. We use $a_i$ in case of linear functions and $h_i$ in case of non-linear functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option C, put 'C')\n",
                "answer2 = '___'\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hint for the function definition below: \n",
                "\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial w_1}\\ =\\ \\frac{\\partial L}{\\partial y}\\ \\frac{\\partial y}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_2}\\ \n",
                "\\frac{\\partial h_2}{\\partial a_1}\n",
                "\\frac{\\partial a_1}{\\partial h_1}\\ \n",
                "\\frac{\\partial h_1}{\\partial w_1}$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial w_2}\\ =\\ \\frac{\\partial L}{\\partial y}\\ \\frac{\\partial y}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_2}\n",
                "\\frac{\\partial h_2}{\\partial w_2}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of loss with respect to w\n",
                "\n",
                "def dldw(W,x):\n",
                "    '''\n",
                "    Now, combine the functions from above and \n",
                "    find the derivative wrt the weights.\n",
                "    These will be for all the points, hence take a mean of all values \n",
                "    for each partial derivative and return as a list of 2 values\n",
                "    '''\n",
                "    h1,h2,a1,y_pred = neural_network(W,x)\n",
                "    w1,w2 = W\n",
                "    \n",
                "    # Derivative of the loss wrt the second weight\n",
                "    dldw2 = ___\n",
                "\n",
                "    # Derivative of the loss wrt the first weight\n",
                "    dldw1 = ___\n",
                "    \n",
                "    return [np.mean(dldw1),np.mean(dldw2)]\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_gradient) ###\n",
                "\n",
                "# Compute the gradient of the loss function with respect to the weights using function defined above\n",
                "# This is dldw() with inputs as 'W' and 'x' as defined above\n",
                "gradW = ___\n",
                "\n",
                "# Print the list of your gradients below\n",
                "print(f'The derivatives of w1 and w2 wrt L are {gradW}')\n",
                ""
            ]
        }
    ]
}
