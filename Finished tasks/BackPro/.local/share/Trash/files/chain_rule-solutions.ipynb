{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the file `backprop.csv`\n",
    "\n",
    "df = pd.read_csv('backprop.csv')\n",
    "\n",
    "x = df.x.values.reshape(-1,1)\n",
    "\n",
    "y = df.y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights, but keep the random seed as 310 for reproducable results\n",
    "\n",
    "np.random.seed(310)\n",
    "W = [np.random.randn(1, 1), np.random.randn(1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the activation function and the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the activation function\n",
    "def A(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# Function to compute the derivative of the activation function\n",
    "def der_A(x):\n",
    "    return np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simple neural network we used in the previous exercise\n",
    "\n",
    "def neural_network(W, x):\n",
    "    \n",
    "    # Computing the first activation\n",
    "    a1 = np.dot(x, W[0])\n",
    "    \n",
    "    # Defining sin() as the activation function\n",
    "    fa1 = A(a1)\n",
    "    \n",
    "    # Computing the second activation\n",
    "    a2 = np.dot(fa1,W[1])\n",
    "    \n",
    "    # Defining sin() as the activation function\n",
    "    y = A(a2)\n",
    "    \n",
    "    return a1,a2,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the chain-rule components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ Look at the schematic in the instructions. If we consider the first weight, what is $\\ \\frac{\\partial a_1}{\\partial w_1}$?\n",
    "\n",
    "- A. $x$\n",
    "- B. $f(x)$\n",
    "- C. $f'(x)$\n",
    "- D. $w_1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option C, put 'C')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of a (particular neuron) with respect to corresponding weight w\n",
    "\n",
    "def dadw(x,firstweight=False):\n",
    "    '''\n",
    "    The derivative of the activation wrt the preceding weight is just the activation of the previous neuron\n",
    "    Note, account for the case where the input layer has no activation layers associated with it. i.e return x if its the first weight \n",
    "    '''\n",
    "    if firstweight == True:\n",
    "        return x\n",
    "    return A(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of h with respect to a\n",
    "\n",
    "def dhda(a):\n",
    "    '''\n",
    "    This is the derivative of the output of the activation function wrt the affine transformation.\n",
    "    Return the derivative of the activation of the affine transformation\n",
    "    '''\n",
    "    \n",
    "    return der_A(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of y with respect to a\n",
    "\n",
    "def dyda(a):\n",
    "    '''\n",
    "    This is the derivative of the output of the neural network wrt the affine transformation.\n",
    "    Return the derivative of the activation of the affine transformation\n",
    "    '''\n",
    "    \n",
    "    return der_A(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of a with respect to h\n",
    "def dadh(w):\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of loss with respect to y\n",
    "def dldy(y_pred,y):\n",
    "    '''\n",
    "    Since our loss function is the MSE,\n",
    "    The partial derivative of L wrt y will be 2*(y_pred - y), for all predictions and response\n",
    "    '''\n",
    "    \n",
    "    return 2*(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ Look at the schematic in the instructions. What is the difference between $h_i$ and $a_i$ for a given layer $i$?\n",
    "\n",
    "- A. $h_i$ and $a_i$ are one and the same\n",
    "- B. $h_i$ is the affine transformation on inputs from layer $i-1$ and $a_i$ is the activation over $h_i$\n",
    "- C. $a_i$ is the affine transformation on inputs from layer $i-1$ and $h_i$ is the activation over $a_i$\n",
    "- D. We use $a_i$ in case of linear functions and $h_i$ in case of non-linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option C, put 'C')\n",
    "answer2 = '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the partial derivate of loss with respect to w\n",
    "\n",
    "def dldw(W,x):\n",
    "    \n",
    "    '''\n",
    "    Now, combine the functions from above and find the derivative wrt weights.\n",
    "    These will be for all the points, hence take a mean of all values for each partial derivative and return as a list of 2 values\n",
    "    \n",
    "    '''\n",
    "    a1,a2,y_pred = neural_network(W,x)\n",
    "    w1,w2 = W\n",
    "    \n",
    "    dldw2 = dldy(y_pred,y)*dyda(a2)*dadw(a1)\n",
    "    dldw1 = dldy(y_pred,y)*dyda(a2)*dadh(w2)*dhda(a1)*dadw(x,firstweight=True)\n",
    "    \n",
    "    return [np.mean(dldw1),np.mean(dldw2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "For the above, remember:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1}\\ =\\ \\frac{\\partial L}{\\partial y}\\ \\frac{\\partial y}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_1}\\ \\frac{\\partial h_1}{\\partial a_1}\\frac{\\partial a_1}{\\partial w_1}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_2}\\ =\\ \\frac{\\partial L}{\\partial y}\\ \\frac{\\partial y}{\\partial a_2}\\frac{\\partial a_2}{\\partial w_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivatives of w1 w2 wrt L are [-0.007777900562982125, 0.2788002123149505]\n"
     ]
    }
   ],
   "source": [
    "### edTest(test_gradient) ###\n",
    "\n",
    "# Compute the gradient of the loss function with respect to the weights using function defined above\n",
    "gradW = dldw(W,x)\n",
    "\n",
    "# Print the list of your gradients below\n",
    "print(f'The derivatives of w1 w2 wrt L are {gradW}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mindchow üç≤\n",
    "\n",
    "1. Compare your computed partial derivatives wrt the previous exercise. Are they the same?\n",
    "\n",
    "2. This example was just for a simple case of 1 neuron in 1 hidden layer. How could we generalize this idea to compute partial derivatives of all the weights?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
