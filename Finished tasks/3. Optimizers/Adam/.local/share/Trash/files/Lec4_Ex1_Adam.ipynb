{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return W**2 -4*W + 4\n",
    "\n",
    "# Get the 500 predictor data points from -2 to 4\n",
    "x = ___\n",
    "\n",
    "# Get the response data using predictor data from the function f above\n",
    "y = ___\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent with adam. Don't forget to bias correct $\\nu$ and $r$ (see notes to the left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_adam_descent) ###\n",
    "\n",
    "# Parameter values suggested in original Adam paper\n",
    "def adam_descent(W, epsilon, rho1=0.9, rho2=0.999, , delta=1e-8):\n",
    "    '''performs gradient descent with Adam optimizer\n",
    "       W is the initial weight value\n",
    "       All default parameters are suggested in original Adam paper except\n",
    "       epsilon which we should think of as the learning rate (try values <= 1)'''\n",
    "\n",
    "\n",
    "    W_prev = None # useful for checking convergence...\n",
    "    v = r = 0 # no 'memory' to start with\n",
    "    t = 0 # iteration counter for bias correction\n",
    "\n",
    "    # append current W to a running array of weight\n",
    "    # values from each iteration\n",
    "    Ws = [W]\n",
    "    while (___): #iterate until converged\n",
    "        # your code here\n",
    "        \n",
    "        # end your code here\n",
    "    return Ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the descent from 6 random starting values for W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view several descents using adam\n",
    "epsilon = ___ # try an epsilon <= 1\n",
    "\n",
    "fig, axes = plt.subplots(2,3, figsize=(10,5))\n",
    "for ax in axes.ravel():\n",
    "    W = np.random.uniform(-2,4) # initial random weight\n",
    "    Ws = adam_descent(W, epsilon=epsilon)\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    ax.scatter(Ws[0], f(Ws[0]), c='green', s=100, label='start')\n",
    "    ax.scatter(np.array(Ws), f(np.array(Ws)), c='r', s=100, alpha=0.1, label='transition')\n",
    "    ax.scatter(Ws[-1], f(Ws[-1]), c='blue', s=100, label='end') # converged value\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After marking the exercise, please go back and change the number of data points once to 50 and once to 10,000. Run it again and check if there is any difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
