{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the gradient_descent function from the helper code\n",
    "from gradient_descent import gradient_descent\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the response data given the predictor data\n",
    "def get_response_data(x):\n",
    "    return np.cos(x) * np.exp(-x/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for performing the gradient descent with Adam\n",
    "\n",
    "# Get the 500 predictor data points from -15 to 15\n",
    "x = ___\n",
    "\n",
    "# Generate the response data from predictor data using function get_response_data above\n",
    "y = ___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated data using the helper code given below\n",
    "plt.plot(x, y, linewidth=3, color='#F5B7B1')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$X$', fontsize=16)\n",
    "plt.ylabel('$Y$', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the derivative\n",
    "def derivative(W):\n",
    "    return (-0.1 * ((np.exp(-W/10))* (10*np.sin(W) + np.cos(W)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform gradient descent with Adam optimizer\n",
    "\n",
    "# Parameter values as suggested in original Adam paper\n",
    "def adam_descent(W, eta=0.01, rho1=0.9, rho2=0.999, delta=1e-8, tolerance=0.000001):\n",
    "\n",
    "    #Variable to store the W value before update. This will help to check for convergence. \n",
    "    W_prev = 0 \n",
    "    \n",
    "    #Inititalise v and r to zero\n",
    "    v = r = 0 \n",
    "    \n",
    "    # t is the iteration counter that will be used in the bias correction equations \n",
    "    t = 0\n",
    "\n",
    "    # Save the current weights to a new list and append the updated weights in each iteration to the same\n",
    "    Ws = [W]\n",
    "    \n",
    "    # Perform the update until convergence\n",
    "    # Convergence is said to have taken place if \n",
    "    # the absolute difference between the previous and updated weight is less than the tolerance\n",
    "    while (___):\n",
    "        \n",
    "        # Increment the counter t for each iteration\n",
    "        ___\n",
    "        \n",
    "        # Compute the gradient of W by calling the derivative function\n",
    "        g = ___  \n",
    "        \n",
    "        # Update the v, the moving average of the gradient according to the equation given in the instructions\n",
    "        v = ___    \n",
    "        \n",
    "        # Update the r, the moving average of the  sqaured gradient according to the equation given in the instructions\n",
    "        r = ___\n",
    "        \n",
    "        # According the the bias correct equations get the corrected v and r values\n",
    "        v_bias_corr = ___     \n",
    "        r_bias_corr = ___     \n",
    "        \n",
    "        # Save the W value in W_prev before update for convergence test\n",
    "        ___                          \n",
    "        \n",
    "        # Update the weight parameters based on the equations given in the instructions\n",
    "        ___    \n",
    "        \n",
    "        # Append the new weight list with the udpated weight value\n",
    "        Ws.append(___)\n",
    "        \n",
    "    return Ws, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_adam_descent) ###\n",
    "\n",
    "# Select an eta value that is less than or equal to 1\n",
    "eta = ___\n",
    "\n",
    "# Plot the descent with Adam starting from 6 random values of W\n",
    "fig, axes = plt.subplots(3,2, figsize=(20,20))\n",
    "\n",
    "# Run the loop for the number of plots to make\n",
    "for ax in axes.ravel():\n",
    "    \n",
    "    # Initialise the weights to a random value between -15 and 15 (which are the min and max value of x) \n",
    "    W = ___\n",
    "    \n",
    "    # Get the weights after gradient descent with Adam and the number of iterations by calling the adam_descent function with the initialised weight and selected epsilon value\n",
    "    Ws, t = ___\n",
    "    \n",
    "    # Helper function to call gradient_descent function and get number of steps\n",
    "    Gs, tg = gradient_descent(W, epsilon=eta) \n",
    "    \n",
    "    # Print the number of steps taken by gradient descent with and without Adam\n",
    "    print(f'The number of steps taken by gradient descent is {tg} and the number of steps taken by GD with Adam is {t}')\n",
    "    \n",
    "    \n",
    "    # Use the helper function below to plot how descent is working with Adam after initialising to random weight\n",
    "    \n",
    "    # Plot the original data\n",
    "    ax.plot(x, y, color='black', alpha=0.6, linewidth=2)\n",
    "    ax.scatter(np.array(Ws), get_response_data(np.array(Ws)), s=150, label='Transition', color='#FDB6AA', alpha=0.6)\n",
    "    \n",
    "    # Plot the starting point\n",
    "    ax.scatter(Ws[0], get_response_data(Ws[0]), c='#009193', s=150, label='Start', alpha=0.5, edgecolor='black')\n",
    "    ax.set_title(f'{t} steps', fontsize=16)\n",
    "    # Plot the ending point\n",
    "    ax.scatter(Ws[-1], get_response_data(Ws[-1]), c='#7A81FF', s=150, label='End',edgecolor='black')\n",
    "    ax.set_xlabel(\"$x$\", fontsize=16)\n",
    "    ax.set_ylabel(\"$y$\", fontsize=16)\n",
    "    ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow üç≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After marking the exercise, please go back and change the number of data points 10,000. Run it again and check if there is any difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
