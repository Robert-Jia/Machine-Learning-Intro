{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessaary libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from gradient_descent import gradient_descent\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the response data given the predictor data\n",
                "def get_response_data(x):\n",
                "    return np.cos(x) * np.exp(-x/10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data for performing the gradient descent with Adam\n",
                "\n",
                "# Get the 500 predictor data points from -15 to 15\n",
                "x = ___\n",
                "\n",
                "# Generate the response data from predictor data using function \n",
                "# get_response_data above\n",
                "y = ___\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the generated data using the helper code given below\n",
                "plt.plot(x, y, linewidth=3, color='#F5B7B1')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.xticks(fontsize=14)\n",
                "plt.yticks(fontsize=14)\n",
                "plt.xlabel('$X$', fontsize=16)\n",
                "plt.ylabel('$Y$', fontsize=16)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the derivative\n",
                "def derivative(W):\n",
                "    return (-0.1 * ((np.exp(-W/10))* (10*np.sin(W) + np.cos(W)) ))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ The bias correction for the first moment is given by which of the following equation? \n",
                "\n",
                "\n",
                "#### A. $\\nu_{bias\\  corr} = \\frac{\\nu}{(t- \\rho_1^t)}$\n",
                "#### B. $\\nu_{bias\\  corr} = \\frac{\\nu^t}{(1- \\rho_1^t)}$\n",
                "#### C. $\\nu_{bias\\  corr} = \\frac{\\nu}{(1 - \\rho_1)}$\n",
                "#### D. $\\nu_{bias\\  corr} = \\frac{\\nu}{(1-\\rho_1^t)}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer1 = '___'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ The bias correction for the second moment is given by which of the following equation? \n",
                "\n",
                "\n",
                "#### A. $r_{bias\\ corr} = \\frac {r}{(1-\\rho_2^{t^2})}$\n",
                "#### B. $r_{bias\\ corr} = \\frac {r}{(1-\\rho_2^t)}$\n",
                "#### C. $r_{bias\\  corr} = \\frac {r^2}{(1-\\rho_2^t)}$\n",
                "#### D. $r_{bias\\  corr} = \\frac{r^2}{(1-\\rho_2^{t^2})}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer2 = '___'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Function to perform gradient descent with Adam optimizer\n",
                "\n",
                "# Parameter values as suggested in original Adam paper\n",
                "def adam_descent(W, eta=0.01, rho1=0.9, rho2=0.999, delta=1e-8, tolerance=0.000001):\n",
                "\n",
                "    #Variable to store the W value before update. This will help to \n",
                "    # check for convergence. \n",
                "    W_prev = 0 \n",
                "    \n",
                "    #Inititalise v and r to zero\n",
                "    v = r = 0 \n",
                "    \n",
                "    # t is the iteration counter that will be used in the \n",
                "    # bias correction equations \n",
                "    t = 0\n",
                "\n",
                "    # Save the current weights to a new list and append the updated \n",
                "    # weights in each iteration to the same\n",
                "    Ws = [W]\n",
                "    \n",
                "    # Perform the update until convergence\n",
                "    # Convergence is said to have taken place if \n",
                "    # the absolute difference between the previous and updated \n",
                "    # weight is less than the tolerance\n",
                "    while (___):\n",
                "        \n",
                "        # Increment the counter t for each iteration\n",
                "        ___\n",
                "        \n",
                "        # Compute the gradient of W by calling the derivative function\n",
                "        g = ___  \n",
                "        \n",
                "        # Update the v, the moving average of the gradient according to \n",
                "        # the equation given in the instructions\n",
                "        v = ___    \n",
                "        \n",
                "        # Update the r, the moving average of the  squared gradient according \n",
                "        # to the equation given in the instructions\n",
                "        r = ___\n",
                "        \n",
                "        # According the the bias correction equations get the \n",
                "        # corrected v and r values\n",
                "        v_bias_corr = ___     \n",
                "        r_bias_corr = ___     \n",
                "        \n",
                "        # Save the W value in W_prev before update for convergence test\n",
                "        ___                          \n",
                "        \n",
                "        # Update the weight parameters based on the equations \n",
                "        # given in the instructions\n",
                "        ___    \n",
                "        \n",
                "        # Append the new weight list with the udpated weight value\n",
                "        Ws.append(___)\n",
                "        \n",
                "    return Ws, t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_adam_descent) ###\n",
                "\n",
                "# Select an eta value that is less than or equal to 1\n",
                "eta = ___\n",
                "\n",
                "# Plot the descent with Adam starting from 6 random values of W\n",
                "fig, axes = plt.subplots(3,2, figsize=(20,20))\n",
                "\n",
                "# Run the loop for the number of plots to make\n",
                "for ax in axes.ravel():\n",
                "    \n",
                "    # Initialise the weights to a random value between -15 and 15 \n",
                "    W = ___\n",
                "    \n",
                "    # Get the weights after gradient descent with Adam and the number of iterations \n",
                "    # by calling the adam_descent function with the initialised weight and selected epsilon value\n",
                "    Ws, t = ___\n",
                "    \n",
                "    # Helper function to call gradient_descent function and get number of steps\n",
                "    Gs, tg = gradient_descent(W, epsilon=eta) \n",
                "    \n",
                "    # Print the number of steps taken by gradient descent with and without Adam\n",
                "    print(f'The number of steps taken by gradient descent is {tg} and the number of steps taken by GD with Adam is {t}')\n",
                "    \n",
                "    \n",
                "    # Use the helper function below to plot how descent is working with Adam after initialising to random weight\n",
                "    \n",
                "    # Plot the original data\n",
                "    ax.plot(x, y, color='black', alpha=0.6, linewidth=2)\n",
                "    ax.scatter(np.array(Ws), get_response_data(np.array(Ws)), s=150, label='Transition', color='#FDB6AA', alpha=0.6)\n",
                "    \n",
                "    # Plot the starting point\n",
                "    ax.scatter(Ws[0], get_response_data(Ws[0]), c='#009193', s=150, label='Start', alpha=0.5, edgecolor='black')\n",
                "    ax.set_title(f'{t} steps', fontsize=16)\n",
                "    # Plot the ending point\n",
                "    ax.scatter(Ws[-1], get_response_data(Ws[-1]), c='#7A81FF', s=150, label='End',edgecolor='black')\n",
                "    ax.set_xlabel(\"$x$\", fontsize=16)\n",
                "    ax.set_ylabel(\"$y$\", fontsize=16)\n",
                "    ax.legend(loc='best');"
            ]
        }
    ]
}
