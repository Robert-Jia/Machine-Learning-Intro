{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import custom class and functions\n",
    "from loss_surface import LossSurface\n",
    "from momentum import Momentum, visualize_descent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute response given predictor\n",
    "def f(x):\n",
    "    return np.cos(3*np.pi*x)/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the derivative\n",
    "def der_f(x):\n",
    "    return -(3*np.pi*x*np.sin(3*np.pi*x)+np.cos(3*np.pi*x))/x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to perform clipping\n",
    "# This function retuns the gradient with a magnitude <=clip_threshold\n",
    "def clip(g, clip_threshold=8):\n",
    "    if np.abs(g) > clip_threshold:\n",
    "        g = g*clip_threshold/np.abs(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get tangent points\n",
    "def get_tangent_line(x, x_range=.5):\n",
    "    y = f(x)\n",
    "    m = der_f(x)\n",
    "    x1, y1 = x, y\n",
    "    x = np.linspace(x1-x_range/2, x1+x_range/2, 50)\n",
    "    y = m*(x-x1)+y1\n",
    "    return x, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the data\n",
    "def plot_it(cur_x, title='', ax=plt):\n",
    "    y = f(x)\n",
    "    ax.plot(x,y)\n",
    "    ax.scatter(cur_x, f(cur_x), c='r', s=80, alpha=1);\n",
    "    x_tan, y_tan, der = get_tangent_line(cur_x)\n",
    "    ax.plot(x_tan, y_tan, ls='--', c='r')\n",
    "    # indicate when if our location is outside the x range\n",
    "    if cur_x > x.max():\n",
    "        ax.axvline(x.max(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/1.6, y.max()/2, x.max()/5, 0, color='r', head_width=.25)\n",
    "    if cur_x < x.min():\n",
    "        ax.axvline(x.min(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/2.5, y.max()/2, -x.max()/5, 0, color='r', head_width=.25)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(-3.5, 3.5)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictor data\n",
    "x = np.linspace(0.1,3, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Which among the following is true about the momentum parameter $\\alpha$ defined here?\n",
    "\n",
    "\n",
    "#### A. As the mometum is decreased, the time to converge most likely increases because the oscillation damping is reduced.\n",
    "#### B. As the mometum is decreased, the time to converge most likely decreases because the weightage given to the previous values is lower.\n",
    "#### C. As the mometum is increased, the time to converge most likely increases because the weightage given to the previous values is higher.\n",
    "#### D. As the mometum is decreased, the time to converge most likely increases because the oscillation damping is increased leading to smoother descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_momentum) ###\n",
    "\n",
    "# Funtion to compute gradient descent with momentum\n",
    "def gradient_descent(cur_x, learning_rate, decay_rate, epsilon, nu, alpha, num_iter)\n",
    "\n",
    "    # Plotting one panel per gradient descent iteration\n",
    "    fig, axs = plt.subplots(num_iter//3, 3, figsize=(15,6), sharey=True)\n",
    "\n",
    "    # To create sub-panels\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        plot_it(cur_x, title=\"{i} step{'' if i == 1 else 's'}\", ax=ax)\n",
    "\n",
    "        # Store the current x value before change in a separate variable\n",
    "        prev_x = ___\n",
    "\n",
    "        # Compute the derivative of the current x\n",
    "        der_cur_x = ___\n",
    "\n",
    "        # Get the gradient of the derivative of x using clipping\n",
    "        delta = ___\n",
    "\n",
    "        # There is no momentum for the first iteration\n",
    "        nu = delta if i == 0 else alpha*nu + (1-alpha)*delta \n",
    "\n",
    "        # Update the x-value using the momentum and learning rate\n",
    "        cur_x = ___\n",
    "\n",
    "        # Update the learning rate based on the decay rate\n",
    "        learning_rate = ___\n",
    "\n",
    "        # Stop algorithm if the change is below threshold\n",
    "        if np.abs(cur_x - prev_x) <= epsilon:\n",
    "            for ax in axs.ravel()[i+1:]:\n",
    "                # Hide unused subplots\n",
    "                ax.axis('off')\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial value of x\n",
    "cur_x = 2.5 \n",
    "\n",
    "# The learning rate for gradient descent\n",
    "learning_rate = 0.35\n",
    "\n",
    "# The decay rate determines the percent by which the learning rate reduces each step\n",
    "decay_rate = 0.5\n",
    "\n",
    "# Setting the epsilon value\n",
    "epsilon = 0.025\n",
    "\n",
    "# Set the initial momentum to zero to indicate no 'memory'\n",
    "nu = 0\n",
    "\n",
    "# Momentum weighting parameter\n",
    "alpha = 0.6\n",
    "\n",
    "# Iterator for plots\n",
    "num_iter = 10\n",
    "\n",
    "# Call the gradient_descent function to perform gradient descent with momentum \n",
    "gradient_descent(cur_x, learning_rate, decay_rate, epsilon, nu, alpha, num_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow 🍲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Try a list of different alpha values with a single learning rate and see how that affects convergence in 2 dimensions\n",
    "\n",
    "learning_rate = ___\n",
    "iteration = 50\n",
    "alphas = ___\n",
    "\n",
    "#Helper code to visualise the data\n",
    "visualize_descent(learning_rate=learning_rate, iteration=iteration, alphas=alphas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
