{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code below to generate the data\n",
    "\n",
    "# Defines the number of data points to generate\n",
    "num_points = 30 \n",
    "\n",
    "# Generate predictor points (x) between 0 and 5\n",
    "x = np.linspace(0,5,num_points)\n",
    "\n",
    "# Generate the response variable (y) using the predictor points\n",
    "y = x * np.sin(x) + np.random.normal(loc=0, scale=1, size=num_points)\n",
    "\n",
    "# Generate data of the true function y = x*sin(x) \n",
    "# x_b will be used for all predictions below \n",
    "x_b = np.linspace(0,5,100)\n",
    "y_b = x_b*np.sin(x_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets with .33 and random_state = 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train data\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "plt.plot(x_train,y_train, '.', label='Train data', markersize=15, color='#FF9A98')\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(x_test,y_test, '.', label='Test data', markersize=15, color='#75B594')\n",
    "\n",
    "# Plot the true data\n",
    "plt.plot(x_b, y_b, '-', label='True function', linewidth=3, color='#5E5E5E')\n",
    "\n",
    "# Set the axes labels\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin with an unregularized NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an unregularized NN. \n",
    "# Initialise the NN, give it an appropriate name for the ease of reading\n",
    "# The FCNN has 5 layers, each with 100 nodes\n",
    "model_1 = models.___\n",
    "\n",
    "# Add 5 hidden layers with 100 neurons each \n",
    "# tanh is the activation for the first layer\n",
    "# relu is the activation for all other layers\n",
    "model_1.add(layers.Dense(___))\n",
    "___\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "# Add the output layer with one neuron and linear activation\n",
    "model_1.add(layers.Dense(___))\n",
    "\n",
    "# View the model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001\n",
    "model_1.compile(___)\n",
    "\n",
    "# Save the history about the model after fitting on the train data\n",
    "# Use 0.2 as the validation split with 1500 epochs and batch size of 10\n",
    "history_1 = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot the data\n",
    "# Plot the MSE of the model\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.title(\"Unregularized model\")\n",
    "plt.semilogy(history_1.history['loss'], label='Train Loss', color='#FF9A98')\n",
    "plt.semilogy(history_1.history['val_loss'],  label='Validation Loss', color='#75B594')\n",
    "plt.legend()\n",
    "\n",
    "# Set the axes labels\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Log MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model above to predict for x_b (used exclusively for plotting)\n",
    "y_pred = ___\n",
    "\n",
    "# Use the model above to predict for x_text  \n",
    "y_pred_test = ___\n",
    "\n",
    "# Compute the test MSE\n",
    "mse = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code to plot the predicted data\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.plot(x_b, y_pred, label = 'Unregularized model', color='#5E5E5E', linewidth=3)\n",
    "plt.plot(x_train,y_train, '.', label='Train data', markersize=15, color='#FF9A98')\n",
    "plt.plot(x_test,y_test, '.', label='Test data', markersize=15, color='#75B594')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularize the NN with L2 regularization \n",
    "#### The previous graph indicates that there is overfitting\n",
    "#### We now initialise a NN with L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now regularise the NN with L2 regularization\n",
    "# Initialise the NN, give it a different name for the ease of reading\n",
    "model_2 = models.___\n",
    "\n",
    "# Add L2 regularization to the model\n",
    "# Use L2 regularization parameter value of 0.1 \n",
    "___\n",
    "\n",
    "# Add 5 hidden layers with 100 neurons each\n",
    "# relu is the activation for all other layers\n",
    "# Don't forget to include the regularization parameter from above\n",
    "model_2.add(layers.Dense(___,  kernel_regularizer=___ , activation=___, input_shape=___)\n",
    "___\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "\n",
    "# Add the output layer with one neuron and linear activation\n",
    "model_2.add(layers.Dense(___))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001\n",
    "model_2.compile(___)\n",
    "\n",
    "# Save the history about the model after fitting on the train data\n",
    "# Use 0.2 as the validation split with 1500 epochs and batch size of 10\n",
    "history_2 = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to plot the MSE of the L2 regularized model\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.title(\"L2 Regularized model\")\n",
    "plt.semilogy(history_2.history['loss'], label='Train Loss', color='#FF9A98')\n",
    "plt.semilogy(history_2.history['val_loss'], label='Validation Loss', color='#75B594')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the regularised model above to predict for x_b (used exclusively for plotting)\n",
    "y_l2_regularized_pred = ___\n",
    "\n",
    "# Use the regularised model above to predict for x_text  \n",
    "y_l2_pred_test = ___\n",
    "\n",
    "# Compute the test MSE by predicting on the test data\n",
    "mse_l2 = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code to plot the predicted data\n",
    "\n",
    "# Plotting the predicted data using the L2 regularized model\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "plt.plot(x_b, y_l2_regularized_pred, label='L2 regularized model', color='#5E5E5E', linewidth=3)\n",
    "\n",
    "# Plotting the predicted data using the unregularized model\n",
    "plt.plot(x_b, y_pred, label = 'Unregularized model', color='#005493', linewidth=2)\n",
    "\n",
    "# Plotting the training data\n",
    "plt.plot(x_train,y_train, '.', label='Train data', markersize=15, color='#FF9A98')\n",
    "\n",
    "# Plotting the testing data\n",
    "plt.plot(x_test,y_test, '.', label='Test data', markersize=15, color='#75B594')\n",
    "\n",
    "# Set the axes labels\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow üç≤\n",
    "\n",
    "**Try the same with L1 regularization. Compare the three neural networks models. Which one best reduces overfitting? Why do you think so?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
