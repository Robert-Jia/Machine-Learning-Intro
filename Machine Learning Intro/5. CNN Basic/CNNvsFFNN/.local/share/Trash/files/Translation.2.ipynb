{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Input\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import helper  # courtesy of CS109B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification: FFNN vs CNN\n",
    "\n",
    "In this exercise we will train a neural network to tell these two images apart.\n",
    "\n",
    "Pavlos          |  Not Pavlos\n",
    ":-------------------------:|:-------------------------:\n",
    "![title](data/pavlos.jpeg) |![title](data/not-pavlos.jpeg)\n",
    "\n",
    "Surely this is a simple task with only two images! But there is a catch. We will use an image generator to create 'translated' versions of our two images. That is, images shifted up or down, left or right. In this way every image the network sees with me a novel variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generator\n",
    "\n",
    "This generator will provide our NNs with randomly translated versions of the above images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Ed/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-def3a00fcd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Our classes: {img_generator.class_indices}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTARGET_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/helper.py\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             width_shift_range=0.3)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     img_generator = data_gen.flow_from_directory(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m--> 524\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Ed/train'"
     ]
    }
   ],
   "source": [
    "img_generator = helper.get_generator()\n",
    "\n",
    "print(f'Our classes: {img_generator.class_indices}')\n",
    "\n",
    "TARGET_SIZE = img_generator.target_size\n",
    "print(f'Generator produces images of size {TARGET_SIZE} (with 3 color channels)')\n",
    "\n",
    "BATCH_SIZE = img_generator.batch_size\n",
    "print(f'Images are generated in batches of size {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = img_generator.next()[0]\n",
    "fig, ax = plt.subplots(4,4)\n",
    "ax = ax.ravel()\n",
    "for i, img in enumerate(sample_batch):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(img)\n",
    "plt.suptitle('Sample Batch of Generated Images', y=1.05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network\n",
    "\n",
    "Our first network will be a feed-forward neural network. The only layers with learned parameters we will be using are dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FFNN = Sequential()\n",
    "# compare input_shape to TARGET_SIZE above\n",
    "FFNN.add(Input(shape=(150, 150, 3)))\n",
    "# fill in the layer needed at the beginning of our FFNN for it to process images\n",
    "# Ex: FFNN.add(Somelayer())  \n",
    "# Hint: check the imports above\n",
    "FFNN.add(__)  \n",
    "# specify a list of the number of nodes for each dense layer\n",
    "# you can try any number of dense layers with any number of nodes in each\n",
    "# Ex: for n_nodes in [a,b,c,..] where x,y,z, etc. are ints\n",
    "for n_nodes in [____]:\n",
    "    FFNN.add(Dense(n_nodes, activation='relu'))\n",
    "FFNN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "FFNN.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do your node and layer number choices affect the number of learned parameters?\n",
    "\n",
    "Can your FFNN do better than chance guessing after 10 epochs?\n",
    "\n",
    "For a real challenge, see if you can do it with fewer than 5 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN_history = FFNN.fit(\n",
    "        img_generator,\n",
    "        steps_per_epoch=300// BATCH_SIZE,\n",
    "        epochs=10,\n",
    "        validation_data=img_generator,\n",
    "        validation_steps=75// BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, name):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "    for i, metric in enumerate(['loss', 'accuracy']):\n",
    "        ax[i].plot(history.history[metric], label='train')\n",
    "        ax[i].plot(history.history[f'val_{metric}'], label='val')\n",
    "        if metric == 'accuracy': ax[i].axhline(0.5, c='r', ls='--', label='trivial accuracy')\n",
    "        ax[i].set_xlabel('epoch')\n",
    "        ax[i].set_ylabel(metric)\n",
    "    plt.suptitle(f'{name} Training', y=1.05)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_history(FFNN_history, 'Feed-Forward Neural Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, FFNN_acc = FFNN.evaluate(img_generator, steps=15)\n",
    "print(f'FFNN Test Accuracy: {FFNN_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "The CNN offers two great advantages over the FFNN in this task:\n",
    "1. Far Fewer Parameters\n",
    "\n",
    "The FFNN had weights between _every_ input pixel and each node in the first dense layer. That's a lot of weights! By contrast, the weights learned by the first CNN layer are not a function of the size of the input image at all. The depend only on the size and number of filters.\n",
    "2. Learning Translation Invariant Features\n",
    "\n",
    "Features are detected by the filters which convolve across the entire image. This means then can recognize the feature they are tuned to no matter where it occurs in an image. The FFNN has no way of representing 'translated' features as being the same. It must learn each position independantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Sequential()\n",
    "CNN.add(Input(shape=(150, 150, 3)))\n",
    "# specify a list of the number of filters for each convolutional layer\n",
    "# you can try any number of convolutional layers with any number of filters in each\n",
    "# Ex: for n_filters in [a,b,c,..] where a,b,c,etc. are ints\n",
    "for n_filters in [____]:\n",
    "    CNN.add(Conv2D(n_filters, kernel_size=3, activation='relu'))\n",
    "    # add a layer to further reduce the dimensionality\n",
    "    # Hint: this layer has no learned parameters of its own\n",
    "    CNN.add(__)\n",
    "# fill in the layer needed between our 2d convolutional layers and the dense layer\n",
    "CNN.add(__)\n",
    "# specify the number of nodes in the dense layer before the output\n",
    "CNN.add(Dense(__, activation='relu'))\n",
    "CNN.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "CNN.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do your choices above affect the number of parameters?\n",
    "\n",
    "Work to get above 95% accuracy after 10 epochs with 100k parameters or fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_history = CNN.fit(\n",
    "        img_generator,\n",
    "        steps_per_epoch=300 // BATCH_SIZE,\n",
    "        epochs=10,\n",
    "        validation_data=img_generator,\n",
    "        validation_steps=75// BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(CNN_history, 'Convolutional Neural Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, CNN_acc = CNN.evaluate(img_generator, steps=15)\n",
    "print(f'CNN Test Accuracy: {CNN_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
