{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca href=\"https://drive.google.com/file/d/1X7lSTQ564-ltEkWgXzyxokmUUnha4QTE/view?usp=sharing\" target=\"_blank\" \u003e\n",
                "  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "from helper import unregularized_model\n",
                "import tensorflow as tf\n",
                "np.random.seed(310)\n",
                "tf.random.set_seed(310)\n",
                "from tensorflow.keras import layers\n",
                "from tensorflow.keras import models\n",
                "from tensorflow.keras import optimizers\n",
                "from tensorflow.keras import regularizers\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implement an unregularized NN "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Call the helper function unregularized_model() to get the unregularized model along with the data\n",
                "x_b, x_train, x_test, y_train, y_test, y_pred, mse = unregularized_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Printing the MSE of the unregularized model\n",
                "print(\"MSE of the unregularized model is\", mse)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implement the NN with batch normalization \n",
                "Build the same network with batch normalization layers after each activation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Building a batch normalization regularized NN. \n",
                "\n",
                "# Initialise the network, give it an appropriate name for the ease of reading\n",
                "# The NN has 5 layers, each with 100 nodes\n",
                "model_2 = models.Sequential(name='BatchNorm')\n",
                "\n",
                "# Add 5 hidden layers with 100 neurons each \n",
                "# tanh is the activation for the first layer\n",
                "# relu is the activation for all other layers\n",
                "\n",
                "# After each dense hidden layer add a simple batch normalization layer with default parameters\n",
                "# The total number of layers here will add to 10\n",
                "model_2.add(layers.Dense(100,  activation='tanh', input_shape=(1,)))\n",
                "___\n",
                "\n",
                "model_2.add(layers.Dense(100,  activation='relu'))\n",
                "___\n",
                "\n",
                "model_2.add(layers.Dense(100,  activation='relu'))\n",
                "___\n",
                "\n",
                "model_2.add(layers.Dense(100,  activation='relu'))\n",
                "___\n",
                "\n",
                "model_2.add(layers.Dense(100,  activation='relu'))\n",
                "___\n",
                "\n",
                "# Add the output layer with one neuron \n",
                "model_2.add(layers.Dense(1,  activation='linear'))\n",
                "\n",
                "# View the model summary\n",
                "model_2.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001\n",
                "model_2.compile(___)\n",
                "\n",
                "# Save the history about the model after fitting on the train data\n",
                "# Use 0.2 validation split  with 1500 epochs and batch size of 10\n",
                "history_2 = ___\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function to plot the data\n",
                "# Plot the MSE of the model\n",
                "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
                "plt.title(\"Batch normalized model\")\n",
                "plt.semilogy(history_2.history['loss'], label='Train Loss', color='#FF9A98', linewidth=1)\n",
                "plt.semilogy(history_2.history['val_loss'],  label='Validation Loss', color='#75B594', linewidth=2)\n",
                "plt.legend()\n",
                "\n",
                "# Set the axes labels\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Log MSE Loss')\n",
                "plt.tight_layout()\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the batch norm implemented model above to predict for x_b (used exclusively for plotting)\n",
                "y_bn_pred = model_2.___\n",
                "\n",
                "# Use the batch norm implemented model above to predict for x_text\n",
                "y_bn_pred_test = model_2.___\n",
                "\n",
                "# Compute the MSE on the test data\n",
                "mse_bn = mean_squared_error(___)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_mse) ###\n",
                "# Print the MSE of the batch normalization regularized model\n",
                "print(\"MSE of the model regularized using batch normalization is\", mse_bn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the helper code to plot the predicted data\n",
                "\n",
                "# Plotting the predicted data using the L2 regularized model\n",
                "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
                "plt.plot(x_b, y_bn_pred, label='Batch normalized model', color='black', linewidth=2)\n",
                "\n",
                "# Plotting the predicted data using the unregularized model\n",
                "plt.plot(x_b, y_pred, label = 'Unregularized model', color='#005493', linewidth=2)\n",
                "\n",
                "# Plotting the training data\n",
                "plt.plot(x_train,y_train, '.', label='Train data', markersize=15, color='#FF9A98')\n",
                "\n",
                "# Plotting the testing data\n",
                "plt.plot(x_test,y_test, '.', label='Test data', markersize=15, color='#75B594')\n",
                "\n",
                "# Set the axes labels\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('Y')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mindchow üç≤\n",
                "\n",
                "\u003cb\u003eAfter marking, make changes to answer the following questions:\n",
                "1. Add a momentum parameter to the batch normalization layers. Pass 0.1 as the value to this parameter. Take a look at both the graphs. Do you notice any change? While value is more efficient?\n",
                "2. Remove all batch normalization layers and add only one with default parameters before the output layer. What changes do you observe?\u003c/b\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Your answer here* "
            ]
        }
    ]
}
