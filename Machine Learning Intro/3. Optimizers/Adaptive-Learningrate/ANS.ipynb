{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessay libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from helper import get_response_data, derivative, lr_decay, rms_prop, adagrad, comparison_plot, landscapes_plot\n",
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get the sample data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data for performing the gradient descent with Adam\n",
                "\n",
                "# Get the 500 predictor data points from -5 to 5\n",
                "x = np.linspace(-5,5,500)\n",
                "\n",
                "# Generate the response data from predictor data using the helper function get_response_data\n",
                "y = get_response_data(x)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize the sample loss landscape\n",
                "\n",
                "Here we assume that $Y$ is the loss, and $X$ is the weight to be updated.\n",
                "\n",
                "${\\partial L}/{\\partial w} = {\\partial y}/{\\partial x}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the helper code below to plot the generated data\n",
                "plt.plot(x, y, linewidth=3, color='#F5B7B1')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.xticks(fontsize=14)\n",
                "plt.yticks(fontsize=14)\n",
                "plt.xlabel('$w$', fontsize=16)\n",
                "plt.ylabel('$L$', fontsize=16)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Effective learning rate over epochs\n",
                "\n",
                "1. RMS Prop\n",
                "2. Exponential Decay\n",
                "3. AdaGrad\n",
                "\n",
                "\n",
                "### Here $\\epsilon$ is the initial learning rate "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the initial learning rate\n",
                "epsilon = 0.1\n",
                "\n",
                "# Set the initial starting value for the weight to a value from -5 to 5\n",
                "W = 1\n",
                "\n",
                "# Set the decay rate for the lr_decay function\n",
                "decay_rate = 0.01"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸  What is likely to happen if you choose a very high decay rate for **Learning Rate Decay**?\n",
                "\n",
                "#### A. The model will converge to the local minima in fewer iterations.\n",
                "#### B. The learning rate will first decrease and then increase. \n",
                "#### C. The model will take more iterations to converge to the local minima.\n",
                "#### D. The learning rate will first increase then decrease."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "# Type your answer within in the quotes given\n",
                "\n",
                "answer1 = 'C'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the helper code below to visualize the variation \n",
                "# of the learning rate over the duration of the train\n",
                "Ws_rms,rms = rms_prop(W, epsilon=epsilon)\n",
                "Ws_ada,ada = adagrad(W, epsilon=epsilon)\n",
                "Ws_lrs, lrs = lr_decay(W,epsilon=epsilon,decay_rate=decay_rate)\n",
                "\n",
                "# To plot the adaptive and scheduled learning rates of the 3 algorithms\n",
                "# The loss landscape plot is for the learning rate decay algorithm\n",
                "comparison_plot(rms=rms,ada=ada,lrs=lrs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function to plot the landscape of the 3 algorithms\n",
                "landscapes_plot(x,y,Ws_lrs,Ws_ada,Ws_rms,rms=rms,ada=ada,lrs=lrs)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ Why does the learning rate for **RMS Prop** first decrease and then increase?\n",
                "\n",
                "#### A. Because of the momentum parameter in RMS prop.\n",
                "#### B. Because of the decreasing gradient as we approach the minima.\n",
                "#### C. Because the second moment is a weighted moving average.\n",
                "#### D. Because of learning rate decay parameter $\\beta$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "# Type your answer within in the quotes given\n",
                "\n",
                "answer2 = 'C'"
            ]
        }
    ]
}
