{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute response given predictor\n",
    "def f(x):\n",
    "    return np.cos(3*np.pi*x)/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute the derivative\n",
    "def der_f(x):\n",
    "    return -(3*np.pi*x*np.sin(3*np.pi*x)+np.cos(3*np.pi*x))/x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to get tangent points\n",
    "def get_tangent_line(x, x_range=.5):\n",
    "    y = f(x)\n",
    "    m = der_f(x)\n",
    "    x1, y1 = x, y\n",
    "    x = np.linspace(x1-x_range/2, x1+x_range/2, 50)\n",
    "    y = m*(x-x1)+y1\n",
    "    return x, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot the data\n",
    "def plot_it(cur_x, title='', ax=plt):\n",
    "    y = f(x)\n",
    "    ax.plot(x,y)\n",
    "    ax.scatter(cur_x, f(cur_x), c='r', s=80, alpha=1);\n",
    "    x_tan, y_tan, der = get_tangent_line(cur_x)\n",
    "    ax.plot(x_tan, y_tan, ls='--', c='r')\n",
    "    # indicate when if our location is outside the x range\n",
    "    if cur_x > x.max():\n",
    "        ax.axvline(x.max(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/1.6, y.max()/2, x.max()/5, 0, color='r', head_width=.25)\n",
    "    if cur_x < x.min():\n",
    "        ax.axvline(x.min(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/2.5, y.max()/2, -x.max()/5, 0, color='r', head_width=.25)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(-3.5, 3.5)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 200 predictor data points between 0.1 and 3 using np.linspace\n",
    "x = ___\n",
    "\n",
    "# Get the response data from the predictor data by calling the function f \n",
    "y = ___\n",
    "\n",
    "# Helper code to plot the generated data\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(x,y, linewidth=3, color='#F5B7B1')\n",
    "plt.xlim(x.min(), x.max());\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$y$', fontsize=16)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to perform clipping\n",
    "# This function should return the gradient with a magnitude <=clip_threshold\n",
    "\n",
    "def clip(g, use_clip=0, clip_threshold=8):\n",
    "    \n",
    "    # Compare the absolute value of the gradient with the clip_threshold\n",
    "    if ___ and use_clip==1:\n",
    "        \n",
    "    # Compute the gradient based on the equation given\n",
    "    ___\n",
    "        \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_clipping) ###\n",
    "# Function to perform gradient descent with clipping and plot it\n",
    "\n",
    "def gradient_descent(cur_x, learning_rate, epsilon, num_iter, use_clip=0):\n",
    "    \n",
    "    # Plotting one panel per gradient descent iteration\n",
    "    fig, axs = plt.subplots(num_iter//3, 3, figsize=(15,6), sharey=True)\n",
    "\n",
    "    # To create sub-panels\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        plot_it(cur_x, title=f\"{i} step{'' if i == 1 else 's'}\", ax=ax)\n",
    "\n",
    "        # Store the current x value before change in a separate variable\n",
    "        prev_x = ___\n",
    "\n",
    "        # Compute the derivative of cur_x using the function der_f\n",
    "        der_cur_x = ___\n",
    "\n",
    "        # Get the gradient of the derivative of x by calling the clip function\n",
    "        g = ___\n",
    "\n",
    "        # Update the x-value using the learning rate\n",
    "        cur_x = ___\n",
    "\n",
    "        # Update the learning rate based on the decay rate\n",
    "        learning_rate = ___\n",
    "\n",
    "        # Stop algorithm if magnitude of change below epsilon\n",
    "        if np.abs(cur_x - prev_x) <= epsilon: \n",
    "            # hide unused subplots\n",
    "            for ax in axs.ravel()[i+1:]:\n",
    "                ax.axis('off') \n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if i == len(axs.ravel())-1:\n",
    "        print('Did not converge!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Set the intial x value to 0.002, learning rate to 0.01, decay rate to 0.09999 and epsilon to 0.0025. With use_clip value as 0 perform gradient descent. What do you obeserve and why?\n",
    "\n",
    "#### A. There is no convergence visible because of high learning rate.\n",
    "#### B. There are 2 convergence points due to small decay rate.\n",
    "#### C. There is no convergence visible because of the lack of clipping.\n",
    "#### D. There is no convergence visible because the epsilon is extremely low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'\n",
    "\"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Which of the following values would you tweak to resolve the above selected issue? Change the code according to the option selected and observe.\n",
    "\n",
    "#### A. Set learning_rate to 0.3\n",
    "#### B. Set decay_rate to 1\n",
    "#### C. Set epsilon to 1\n",
    "#### D. Set use_clip to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer2 = '___'\n",
    "\"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the initial value of x\n",
    "cur_x = ___\n",
    "\n",
    "# The learning rate for gradient descent\n",
    "learning_rate = ___\n",
    "\n",
    "# The decay rate determines the percent by which the learning rate reduces each step\n",
    "decay_rate = ___\n",
    "\n",
    "# Setting the epsilon value\n",
    "epsilon = ___\n",
    "\n",
    "# The number of panels to show\n",
    "num_iter = ___\n",
    "\n",
    "# Call the gradient_descent function with appropriate parameters to perform gradient descent and plot the graph\n",
    "gradient_descent(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Run the code again by setting the intial x value to 0.52, learning rate to 0.01, decay rate to 0.000009999 and epsilon to 0.0025. With use_clip value as 1 perform gradient descent. What do you obeserve and why?\n",
    "\n",
    "#### A. For a sufficinelty large num_iter there is no convergence because of high learning rate.\n",
    "#### B. The rate of convergence is high because of the reduced decay rate.\n",
    "#### C. For a sufficiently large num_iter there is convergence because of clipping.\n",
    "#### D. The rate of convergence is high because of the ideal initial x-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer3 = '___'\n",
    "\"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial value of x\n",
    "cur_x = ___\n",
    "\n",
    "# The learning rate for gradient descent\n",
    "learning_rate = ___\n",
    "\n",
    "# The decay rate determines the percent by which the learning rate reduces each step\n",
    "decay_rate = ___\n",
    "\n",
    "# Setting the epsilon value\n",
    "epsilon = ___\n",
    "\n",
    "# The number of panels to show\n",
    "num_iter = ___\n",
    "\n",
    "# Call the gradient_descent function with appropriate parameters to perform gradient descent and plot the graph\n",
    "gradient_descent(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Which of the following values would you ideally tweak to resolve the above selected issue? Change the code according to the option selected and observe.\n",
    "\n",
    "#### A. Increase the decay_rate\n",
    "#### B. Reduce epsilon\n",
    "#### C. Set cur_x to 1.5\n",
    "#### D. Increase num_iter to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow4) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer4 = '___'\n",
    "\"A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ What will happen if the decay rate is made extremely small, for example decay_rate=0.000001?\n",
    "\n",
    "#### A. The learning rate will reduce rapidly to become zero and thus each value would be the same as the previous.\n",
    "#### B. The learning rate will increase rapidly causing to jump over the minima.\n",
    "#### C. The learning rate will hardly change and hence would take the algorithm longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow5) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer5 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow 🍲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After marking your exercise, try tweaking learning_rate, and perhaps the default value of clip_threshold. Can you anticipate how it will affect your results? \n",
    "\n",
    "See if you can find a combination that will converge to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow6) ###\n",
    "# Type your answer within in the quotes given\n",
    "\n",
    "answer6 = '___'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
