{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
                "from tensorflow.keras.preprocessing import image\n",
                "from tensorflow.keras.models import Model\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import cv2\n",
                "import pickle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the MobileNet V2 pre-trained model\n",
                "# Rather than training a model from scratch we can use a pre trained\n",
                "# model that has already been trained in the imagenet dataset\n",
                "# MobileNetV2 is a SOTA model for image classification \n",
                "model = MobileNetV2(weights='imagenet')\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "\n",
                "# Find the last convolutional layer\n",
                "# Inspect the model summary and find the last convolution layer\n",
                "# Get the name of the last convolution layer\n",
                "conv_layer_name = model.layers[___].name\n",
                "print(conv_layer_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Take a sample image to find the saliency map\n",
                "img_path = './cat.png'\n",
                "\n",
                "# Load the image with the target_size for mobilenet\n",
                "img = image.load_img(img_path, target_size=(224, 224))\n",
                "\n",
                "# Convert the image to a numpy array \n",
                "x = image.img_to_array(img)\n",
                "\n",
                "# Add an extra dimension for batch size \n",
                "# to change it to (1,224,224,3)\n",
                "x = np.expand_dims(x, axis=0)\n",
                "\n",
                "# Use the MobileNetV2 preprocess_input function on the image\n",
                "x = preprocess_input(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the pretrained model to make a prediction \n",
                "preds = model.predict(x)\n",
                "\n",
                "# Useful dictionary to go from label index to actual label\n",
                "with open('idx2name.pkl', 'rb') as handle:\n",
                "    keras_idx_to_name = pickle.load(handle)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# See what the output predictions is:\n",
                "prediction_class = keras_idx_to_name[np.argmax(preds,axis=1).item(0)]\n",
                "print(f'Prediction class is {prediction_class}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We use the tf.keras Functional API to get \n",
                "# 1. The model prediction probabilities\n",
                "# 2. The feature maps after the last convolution in the model\n",
                "\n",
                "# Get the last convolution layer in the network\n",
                "last_conv_layer = model.get_layer(conv_layer_name)\n",
                "\n",
                "# Get the output predictions and the last_conv_layer\n",
                "# Using tf.keras functional API\n",
                "get_maps = Model(inputs = [model.inputs], outputs = [model.output, last_conv_layer.output])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now we perform the Grad-CAM,\n",
                "# We take the gradient of the output with respect to the feature maps\n",
                "# after the convolution \n",
                "with tf.GradientTape() as tape:\n",
                "\n",
                "    # Getting the required outputs \n",
                "    model_out, last_conv_layer = get_maps(x)\n",
                "    \n",
                "    # We choose the output with maximum probability\n",
                "    # But this can be different depending on your choice\n",
                "    # For eg. you could select the second highest probability value \n",
                "    class_out = tf.reduce_max(model_out)\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚è∏ **Take the gradients**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "\n",
                "# We take the gradients \n",
                "# tape.gradient() takes the gradient of something with respect to \n",
                "# something else. Here we want the derivative of the output class\n",
                "# with respect to to the last conv layer\n",
                "grads = tape.gradient(___)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Here we combine all the gradients for each feature map \n",
                "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
                "  \n",
                "# As per grad-CAM literature, here we need to multiply \n",
                "# the pooled grads with each feature map and take the average across\n",
                "# all the feature maps to make the heat map\n",
                "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Below we convert heatmap to numpy\n",
                "# Make all values positive\n",
                "# and reshape from (1,7,7) to (7,7) for ease of plotting\n",
                "heatmap = heatmap.numpy()\n",
                "heatmap[heatmap \u003c 0] = 0 #relu\n",
                "heatmap = (heatmap - heatmap.min())/(heatmap.max() - heatmap.min())\n",
                "heatmap = heatmap.reshape((7, 7))\n",
                "\n",
                "# We plot the (7,7) heatmap\n",
                "plt.imshow(heatmap,cmap='jet')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inorder to map to the original image\n",
                "# This heatmap has to be be resized\n",
                "resized_heatmap = np.uint8(cv2.resize(heatmap,(224,224))*255)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We need to add a pre-processing step\n",
                "# to convert the grayscale heatmap\n",
                "# to a true JET colormap of 3 channels\n",
                "# for ease of viewing\n",
                "val = np.uint8(256-resized_heatmap)\n",
                "heatmap_final = cv2.applyColorMap(val, cv2.COLORMAP_JET)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We also prepare the image for plotting\n",
                "# by converting to tensor\n",
                "# and converting dtype to int8\n",
                "img = image.img_to_array(img)\n",
                "img = np.uint8(img)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Finally, we use the cv2.addweighted function\n",
                "# to superimpose the heatmap on the original image\n",
                "# Use the helper code below to do the same\n",
                "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
                "ax.imshow(cv2.addWeighted(heatmap_final, 0.5, img, 0.5, 0))\n",
                "ax.axis('off');\n",
                "fig.suptitle(f'Predicted class: {prediction_class}',y=0.92,fontsize=14);\n",
                "plt.show();"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚è∏ Will Grad-CAM work if we took the output from the last ReLU instead ? (True or False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow3) ###\n",
                "\n",
                "# Type your answer within in the quotes given \n",
                "answer3 = '___'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚è∏ The heatmap output is displaying:\n",
                "\n",
                "A: The weights of the layer \u003cbr/\u003e\n",
                "\n",
                "B: A 7x7 mask from the input image \u003cbr/\u003e\n",
                "\n",
                "C: The pixels that activates the most in red and the least in blues \u003cbr/\u003e \n",
                "\n",
                "D: A feature map of the input image \u003cbr/\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow4) ###\n",
                "\n",
                "# Type your answer within in the quotes given \n",
                "answer4 = '_'"
            ]
        }
    ]
}
