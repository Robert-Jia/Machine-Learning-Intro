# This document adds simple description about each paper
## [Paper 1: On the Capabilities of Multilayer Perceptrons](https://github.com/Z-Robert-Jia/Machine-Learning-Intro/blob/main/Papers/%20On%20the%20Capabilities%20of%20Multilayer%20Perceptrons.pdf)
This Paper summarize penty of previous studies, and discuss about how multilayer perceptrons could approximate any continuous functions efficiently. 

## [Paper 2: SGD](https://github.com/Z-Robert-Jia/Machine-Learning-Intro/blob/main/Papers/SGD.pdf)
This paper Provides a quick introducting about SGD, how it is effective, and model tuning tricks. E.g. Bopt ∝ εN; Bopt ∝ 1/(1 − m) (Bopt being the batch size)

## [Paper 3: Gradient_Descent](https://github.com/Z-Robert-Jia/Machine-Learning-Intro/blob/main/Papers/Gradient_Descent.pdf)
This paper introduces all imortant concepts about gradient descent, especially on how to choose and adjust hyperparameters. 

## [Paper 4: Automatic_Differentiation](https://github.com/Z-Robert-Jia/Machine-Learning-Intro/blob/main/Papers/Automatic_Differentiation.pdf)
This paper introduces ideas around confusing concepts like 'automatic differentiation', 'audo-diff', 'symbolic differentiation' etc. Meanwhile, it points out automatic differentiation's wide application in machine learning and it's great potential. 




## [Paper 5: Unet3D](https://arxiv.org/abs/1606.06650)
